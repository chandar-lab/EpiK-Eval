--- original_data_loader.py	2023-11-29 16:40:59.455983011 -0500
+++ modified_data_loader.py	2023-11-29 14:24:35.371044134 -0500
@@ -677,25 +677,38 @@
                 if self._stop_iteration and next_batch_info[0] is None:
                     stop_iteration = True
 
-            if not self._drop_last and stop_iteration and observed_batch_size % self.state.num_processes != 0:
-                # If the last batch is not complete, let's add the first batch to it.
-                batch = concatenate([batch, first_batch], dim=0)
-                # Batch size computation above is wrong, it's off by 1 so we fix it.
-                batch_size += 1
-
-            data_slice = slice(self.state.process_index * batch_size, (self.state.process_index + 1) * batch_size)
-            batch = self.slice_fn(
-                batch,
-                data_slice,
-                process_index=self.state.process_index,
-                num_processes=self.state.num_processes,
-            )
+            if (observed_batch_size % self.state.num_processes) != 0:
+                number_missing_samples = self.state.num_processes - (observed_batch_size % self.state.num_processes)
+                num_repeats = math.ceil((observed_batch_size + number_missing_samples) / observed_batch_size)
+                new_batch_size = observed_batch_size + number_missing_samples
+
+                repeated_batch = []
+                for tensor in batch:
+                    repeated_shape = [1] * len(tensor.shape)
+                    repeated_shape[0] = num_repeats
+                    tensor = tensor.repeat(repeated_shape)[:new_batch_size]
+                    repeated_batch.append(tensor)
+
+                nonduplicate_mask = torch.ones(new_batch_size, dtype=torch.bool, device=batch[0].device)
+                nonduplicate_mask[observed_batch_size:] = 0
+
+                repeated_batch.append(nonduplicate_mask)
+                batch = repeated_batch
+            else:
+                nonduplicate_mask = torch.ones(batch[0].shape[0], dtype=torch.bool, device=batch[0].device)
+                batch = list(batch)
+                batch.append(nonduplicate_mask)
+
+            # Take batch slice corresponding to process
+            assert (batch[0].shape[0] % self.state.num_processes) == 0
+            device_batch_size = batch[0].shape[0] // self.state.num_processes
+            device_batch = [tensor[self.state.process_index * device_batch_size : (self.state.process_index + 1) * device_batch_size] for tensor in batch]
 
             if stop_iteration:
                 self.end_of_dataloader = True
                 self.remainder = observed_batch_size
             if batch_index >= self.skip_batches:
-                yield batch
+                yield device_batch
             batch_index += 1
         self.iteration += 1
         self.end()
