{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4769c7fe",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "This script takes a model's answer log as input and computes the metrics presented in the paper: https://arxiv.org/abs/2310.15372\n",
    "\n",
    "Set the correct path to the csv log below in the [Hyperparameters](#Hyperparameters) section and then run the remaining cells to get the metrics for that log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce122562",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd54e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = '../logs/flan-t5-xl_unsegmented_epoch2000.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ff51b",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Do not modify the following lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Number of statements to ignore at the end of an answer, for each task. This is to ignore reasoning parts and the final answer and solely count hallucinations in the statements to recall.\n",
    "STATEMENTS_TO_IGNORE = {1: 1,\n",
    "                        2: 1,\n",
    "                        3: 1,\n",
    "                        4: 2,\n",
    "                        5: 2,\n",
    "                        6: 1,\n",
    "                        7: 1,\n",
    "                        8: 2,\n",
    "                        9: 2,\n",
    "                        10: 1,\n",
    "                        11: 1,\n",
    "                        12: 1,\n",
    "                        13: 2,\n",
    "                        14: 1,\n",
    "                        15: 1,\n",
    "                        16: 1,\n",
    "                        17: 1,\n",
    "                        18: 2}\n",
    "REASONING_KEYWORDS = {4: ['before', 'after'],\n",
    "                       5: ['Those are'],\n",
    "                       8: ['='],\n",
    "                       9: ['='],\n",
    "                       13: ['is the'],\n",
    "                       18: ['=']} # keywords that should be in the reasoning statement for each respective task\n",
    "\n",
    "answer_csv = pd.read_csv(CSV, sep='|')\n",
    "train_answers = answer_csv[answer_csv['set'] == 'train']\n",
    "test_answers = answer_csv[answer_csv['set'] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581517f6",
   "metadata": {},
   "source": [
    "# Train Hallucination Rate / Memorization\n",
    "When prompted with the title of a story (unsegmented setup) or the title of a story part (segmented setup), the model is expected to output, respectively, the story or the story part. We take the model's output and count the number of sentences that aren't in the actual story or story part (again respectively) over the total number of sentences in the model's output. This can be interpreted as the probability of a sentence being incorrect, the hallucination rate at the sentence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucinations = 0\n",
    "total_sentences = 0\n",
    "hallucinations_per_task = {}\n",
    "total_sentences_per_task = {}\n",
    "\n",
    "for task, target_answer, model_answer in zip(train_answers['task'], train_answers['target answer'], train_answers['model answer']):\n",
    "    if task not in hallucinations_per_task:\n",
    "        hallucinations_per_task[task] = 0\n",
    "        total_sentences_per_task[task] = 0\n",
    "    \n",
    "    if '\\n' in target_answer: # some tokenizers replace \\n with the space character and the sentences in the training samples are separated by '\\n'\n",
    "        target_answer = target_answer.split('.\\n')\n",
    "        model_answer = model_answer.split('.\\n')\n",
    "    else:\n",
    "        target_answer = target_answer.split('. ')\n",
    "        model_answer = model_answer.split('. ')\n",
    "        \n",
    "    # Measure hallucination\n",
    "    for sentence in model_answer:\n",
    "        if sentence not in target_answer:\n",
    "            hallucinations += 1\n",
    "            hallucinations_per_task[task] += 1\n",
    "    total_sentences += len(model_answer)\n",
    "    total_sentences_per_task[task] += len(model_answer)\n",
    "        \n",
    "print(f'Train hallucination rate: {100 * hallucinations/total_sentences:.2f}%')\n",
    "for task in sorted(total_sentences_per_task.keys()):\n",
    "    print(f'[Task {task}] Train hallucination rate: {100 * hallucinations_per_task[task]/total_sentences_per_task[task]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91da041",
   "metadata": {},
   "source": [
    "# Test Accuracy\n",
    "We report the following metrics with respect to the test q/a:\n",
    "- Whole Answer Accuracy: Percentage of model answers that match the target.\n",
    "- Recall Accuracy: Percentage of recall parts in the model answer that match the target.\n",
    "- Reasoning Accuracy: Percentage of reasoning parts in the model answer that match the target, only considering the set of answers where the recall was correct.\n",
    "- Final Answer Accuracy: Percentage of final answers that match the target, only considering the set of answers where the recall and (if there is a reasoning part in the task) reasoning was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6112dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_whole = 0\n",
    "correct_recall = 0\n",
    "correct_reasoning = 0\n",
    "correct_final_answer = 0\n",
    "total_reasoning = 0\n",
    "total_final_answer = 0\n",
    "correct_whole_per_task = {}\n",
    "correct_recall_per_task = {}\n",
    "correct_reasoning_per_task = {}\n",
    "correct_final_answer_per_task = {}\n",
    "total_answers_per_task = {}\n",
    "total_reasoning_per_task = {}\n",
    "total_final_answer_per_task = {}\n",
    "for task in STATEMENTS_TO_IGNORE.keys():\n",
    "    correct_whole_per_task[task] = 0\n",
    "    correct_recall_per_task[task] = 0\n",
    "    correct_reasoning_per_task[task] = 0\n",
    "    correct_final_answer_per_task[task] = 0\n",
    "    total_answers_per_task[task] = 0\n",
    "    total_reasoning_per_task[task] = 0\n",
    "    total_final_answer_per_task[task] = 0\n",
    "\n",
    "for task, target_answer, model_answer in zip(test_answers['task'], test_answers['target answer'], test_answers['model answer']):\n",
    "    total_answers_per_task[task] += 1\n",
    "    if model_answer == target_answer:\n",
    "        correct_whole += 1\n",
    "        correct_whole_per_task[task] += 1\n",
    "\n",
    "    target_answer = target_answer.split('. ')\n",
    "    target_recall_part = '. '.join(target_answer[:-STATEMENTS_TO_IGNORE[task]])\n",
    "    target_reasoning_part = '. '.join(target_answer[-STATEMENTS_TO_IGNORE[task]:-1]) if STATEMENTS_TO_IGNORE[task] > 1 else None\n",
    "    target_final_part = target_answer[-1]\n",
    "    \n",
    "    model_answer = model_answer.split('. ')\n",
    "    if STATEMENTS_TO_IGNORE[task] == 1: # no reasoning part, only final answer to remove if present\n",
    "        model_reasoning_part = None\n",
    "        if 'The answer is' in model_answer[-1]:\n",
    "            model_recall_part = '. '.join(model_answer[:-1])\n",
    "            model_final_part = model_answer[-1]\n",
    "        else:\n",
    "            model_recall_part = '. '.join(model_answer)\n",
    "            model_final_part = None\n",
    "    elif STATEMENTS_TO_IGNORE[task] > 1: # reasoning and final part need to be remove if present\n",
    "        answer_has_reasoning_part = False\n",
    "        answer_has_final_part = False\n",
    "        for keyword in REASONING_KEYWORDS[task]:\n",
    "            for sentence in model_answer[-STATEMENTS_TO_IGNORE[task]:]:\n",
    "                if keyword in sentence:\n",
    "                    answer_has_reasoning_part = True\n",
    "                if 'The answer is' in sentence:\n",
    "                    answer_has_final_part = True\n",
    "        \n",
    "        statements_to_ignore = 0\n",
    "        if answer_has_reasoning_part:\n",
    "            statements_to_ignore += STATEMENTS_TO_IGNORE[task] - 1\n",
    "        if answer_has_final_part:\n",
    "            statements_to_ignore += 1\n",
    "        model_recall_part = '. '.join(model_answer[:-statements_to_ignore])\n",
    "        \n",
    "        if answer_has_reasoning_part:\n",
    "            if answer_has_final_part:\n",
    "                model_reasoning_part = '. '.join(model_answer[-STATEMENTS_TO_IGNORE[task]:-1])\n",
    "            else:\n",
    "                model_reasoning_part = '. '.join(model_answer[(-STATEMENTS_TO_IGNORE[task]) + 1:])\n",
    "        else:\n",
    "            model_reasoning_part = None\n",
    "            \n",
    "        if answer_has_final_part:\n",
    "            model_final_part = model_answer[-1]\n",
    "        else:\n",
    "            model_final_part = None\n",
    "    \n",
    "    if model_recall_part == target_recall_part:\n",
    "        correct_recall += 1\n",
    "        correct_recall_per_task[task] += 1\n",
    "        \n",
    "        # When recall is correct, check reasoning if there is one\n",
    "        if target_reasoning_part is not None:\n",
    "            total_reasoning += 1\n",
    "            total_reasoning_per_task[task] += 1\n",
    "            if model_reasoning_part == target_reasoning_part:\n",
    "                correct_reasoning += 1\n",
    "                correct_reasoning_per_task[task] += 1\n",
    "\n",
    "                # When reasoning is correct, check final answer\n",
    "                total_final_answer += 1\n",
    "                total_final_answer_per_task[task] += 1\n",
    "                if model_final_part == target_final_part:\n",
    "                    correct_final_answer += 1\n",
    "                    correct_final_answer_per_task[task] += 1\n",
    "        else:\n",
    "            # When recall is correct and there is no reasoning part, check final answer\n",
    "            total_final_answer += 1\n",
    "            total_final_answer_per_task[task] += 1\n",
    "            if model_final_part == target_final_part:\n",
    "                correct_final_answer += 1\n",
    "                correct_final_answer_per_task[task] += 1\n",
    "\n",
    "print(f'Whole Answer Accuracy: {100 * correct_whole/len(test_answers):.2f}%, '\\\n",
    "      f'Recall Accuracy: {100 * correct_recall/len(test_answers):.2f}%, '\\\n",
    "      f'Reasoning Accuracy: {100 * correct_reasoning/total_reasoning:.2f}%, '\\\n",
    "      f'Final Answer Accuracy: {100 * correct_final_answer/total_final_answer:.2f}%')\n",
    "for task in sorted(correct_whole_per_task.keys()):\n",
    "    output = f'[Task {task}] Whole Answer Accuracy: {100 * correct_whole_per_task[task]/total_answers_per_task[task]:.2f}%, ' \\\n",
    "        f'Recall Accuracy: {100 * correct_recall_per_task[task]/total_answers_per_task[task]:.2f}%, '\n",
    "    \n",
    "    if STATEMENTS_TO_IGNORE[task] > 1: # means there is a reasoning part to the answer\n",
    "        if total_reasoning_per_task[task] > 0:\n",
    "            output += f'Reasoning Accuracy: {100 * correct_reasoning_per_task[task]/total_reasoning_per_task[task]:.2f}%, '\n",
    "        else:\n",
    "            output += f'Reasoning Accuracy: 0.00%, '\n",
    "            \n",
    "    if total_final_answer_per_task[task] == 0: # All recalls failed case, we report 0 accuracy for the final answer to avoid dividing by zero\n",
    "        output += f'Final Answer Accuracy: 0.00%'\n",
    "    else:\n",
    "        output += f'Final Answer Accuracy: {100 * correct_final_answer_per_task[task]/total_final_answer_per_task[task]:.2f}%'\n",
    "            \n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca399ce",
   "metadata": {},
   "source": [
    "# Test Hallucination Rate\n",
    "The hallucination rate for the test q/a is the percentage of sentences in the recall part of the model answer that aren't in the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4886db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucinations = 0\n",
    "total_sentences = 0\n",
    "hallucinations_per_task = {}\n",
    "total_sentences_per_task = {}\n",
    "for task in STATEMENTS_TO_IGNORE.keys():\n",
    "    hallucinations_per_task[task] = 0\n",
    "    total_sentences_per_task[task] = 0\n",
    "\n",
    "for task, target_answer, model_answer in zip(test_answers['task'], test_answers['target answer'], test_answers['model answer']):    \n",
    "    target_recall_part = target_answer.split('. ')[:-STATEMENTS_TO_IGNORE[task]]\n",
    "    model_answer = model_answer.split('. ')\n",
    "    if STATEMENTS_TO_IGNORE[task] == 1: # no reasoning part, only final answer to remove if present\n",
    "        if 'The answer is' in model_answer[-1]:\n",
    "            model_recall_part = model_answer[:-1]\n",
    "        else:\n",
    "            model_recall_part = model_answer\n",
    "    elif STATEMENTS_TO_IGNORE[task] > 1: # reasoning and final part need to be remove if present\n",
    "        answer_has_reasoning_part = False\n",
    "        answer_has_final_part = False\n",
    "        for keyword in REASONING_KEYWORDS[task]:\n",
    "            for sentence in model_answer[-STATEMENTS_TO_IGNORE[task]:]:\n",
    "                if keyword in sentence:\n",
    "                    answer_has_reasoning_part = True\n",
    "                if 'The answer is' in sentence:\n",
    "                    answer_has_final_part = True\n",
    "        \n",
    "        statements_to_ignore = 0\n",
    "        if answer_has_reasoning_part:\n",
    "            statements_to_ignore += STATEMENTS_TO_IGNORE[task] - 1\n",
    "        if answer_has_final_part:\n",
    "            statements_to_ignore += 1\n",
    "        model_recall_part = model_answer[:-statements_to_ignore]\n",
    "    \n",
    "    # Measure hallucination\n",
    "    for sentence in model_recall_part:\n",
    "        if sentence not in target_recall_part:\n",
    "            hallucinations += 1\n",
    "            hallucinations_per_task[task] += 1\n",
    "    total_sentences += len(model_recall_part)\n",
    "    total_sentences_per_task[task] += len(model_recall_part)\n",
    "        \n",
    "print(f'Test hallucination rate: {100 * hallucinations/total_sentences:.2f}%')\n",
    "for task in sorted(total_sentences_per_task.keys()):\n",
    "    print(f'[Task {task}] Test hallucination rate: {100 * hallucinations_per_task[task]/total_sentences_per_task[task]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce08ab49",
   "metadata": {},
   "source": [
    "# Test Answer length distribution\n",
    "Comparison of answer length distribution (recall part only) between model and target, where length is in number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aea331",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_len = []\n",
    "model_len = []\n",
    "target_len_per_task = {}\n",
    "model_len_per_task = {}\n",
    "for task in STATEMENTS_TO_IGNORE.keys():\n",
    "    target_len_per_task[task] = []\n",
    "    model_len_per_task[task] = []\n",
    "\n",
    "for task, target_answer, model_answer in zip(test_answers['task'], test_answers['target answer'], test_answers['model answer']):\n",
    "    target_answer = target_answer.split('. ')[: - STATEMENTS_TO_IGNORE[task]]\n",
    "    model_answer = model_answer.split('. ')\n",
    "    if STATEMENTS_TO_IGNORE[task] == 1: # no reasoning part, only final answer to remove if present\n",
    "        if 'The answer is' in model_answer[-1]:\n",
    "            model_answer = model_answer[:-1]\n",
    "    elif STATEMENTS_TO_IGNORE[task] > 1: # reasoning and final part need to be remove if present\n",
    "        answer_has_reasoning_part = False\n",
    "        answer_has_final_part = False\n",
    "        for keyword in REASONING_KEYWORDS[task]:\n",
    "            for sentence in model_answer[-STATEMENTS_TO_IGNORE[task]:]:\n",
    "                if keyword in sentence:\n",
    "                    answer_has_reasoning_part = True\n",
    "                if 'The answer is' in sentence:\n",
    "                    answer_has_final_part = True\n",
    "        \n",
    "        statements_to_ignore = 0\n",
    "        if answer_has_reasoning_part:\n",
    "            statements_to_ignore += STATEMENTS_TO_IGNORE[task] - 1\n",
    "        if answer_has_final_part:\n",
    "            statements_to_ignore += 1\n",
    "        if statements_to_ignore > 0:\n",
    "            model_answer = model_answer[:-statements_to_ignore]\n",
    "        \n",
    "    target_len.append(len(target_answer))\n",
    "    model_len.append(len(model_answer))\n",
    "    target_len_per_task[task].append(len(target_answer))\n",
    "    model_len_per_task[task].append(len(model_answer))\n",
    "    \n",
    "target_lengths, target_counts = np.unique(np.asarray(target_len), return_counts=True)\n",
    "output = ['Target Count per Length']\n",
    "for _len, count in zip(target_lengths, target_counts):\n",
    "    output.append(f'{_len}: {count}')\n",
    "print(', '.join(output))\n",
    "    \n",
    "model_lengths, model_counts = np.unique(np.asarray(model_len), return_counts=True)\n",
    "output = ['Model Count per Length']\n",
    "for _len, count in zip(model_lengths, model_counts):\n",
    "    output.append(f'{_len}: {count}')\n",
    "print(', '.join(output))\n",
    "\n",
    "for task in sorted(target_len_per_task.keys()):\n",
    "    target_lengths, target_counts = np.unique(np.asarray(target_len_per_task[task]), return_counts=True)\n",
    "    output = [f'[Task {task}] Target Count per Length']\n",
    "    for _len, count in zip(target_lengths, target_counts):\n",
    "        output.append(f'{_len}: {count}')\n",
    "    print(', '.join(output))\n",
    "\n",
    "    model_lengths, model_counts = np.unique(np.asarray(model_len_per_task[task]), return_counts=True)\n",
    "    output = [f'[Task {task}] Model Count per Length']\n",
    "    for _len, count in zip(model_lengths, model_counts):\n",
    "        output.append(f'{_len}: {count}')\n",
    "    print(', '.join(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
